import streamlit as st
import PyPDF2
import re
import requests
import time
import json
import pandas as pd
import numpy as np
import networkx as nx
from pyvis.network import Network
from streamlit.components.v1 import html
from io import BytesIO, StringIO
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import altair as alt
from bs4 import BeautifulSoup
import concurrent.futures
from urllib.parse import urljoin, urlparse
import base64
from wordcloud import WordCloud
import bibtexparser
from unidecode import unidecode
import logging
import os

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(levelname)s - %(message)s')

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤ Streamlit
st.set_page_config(page_title="‡∏ï‡∏±‡∏ß‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á", layout="wide")

# ‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
if "current_doc" not in st.session_state:
    st.session_state.current_doc = None
if "references" not in st.session_state:
    st.session_state.references = []
if "doi_references" not in st.session_state:
    st.session_state.doi_references = []
if "graph_html" not in st.session_state:
    st.session_state.graph_html = None
if "doc_history" not in st.session_state:
    st.session_state.doc_history = {}
if "progress_bar" not in st.session_state:
    st.session_state.progress_bar = st.empty()
if "author_network" not in st.session_state:
    st.session_state.author_network = None
if "keyword_data" not in st.session_state:
    st.session_state.keyword_data = None
if "pdf_extracts" not in st.session_state:
    st.session_state.pdf_extracts = {}
if "language" not in st.session_state:
    st.session_state.language = "th"  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
if "cache_dir" not in st.session_state:
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Ñ‡∏ä ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
    cache_dir = ".cache"
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
    st.session_state.cache_dir = cache_dir

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤
def get_ui_text(key):
    texts = {
        "title": {
            "th": "‡∏ï‡∏±‡∏ß‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á",
            "en": "Reference Document Analyzer"
        },
        "upload_prompt": {
            "th": "‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á",
            "en": "Upload PDF file with references"
        },
        "clear_button": {
            "th": "‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
            "en": "Clear Data"
        },
        "references_list": {
            "th": "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á",
            "en": "References List"
        },
        "urls_section": {
            "th": "‡∏•‡∏¥‡∏á‡∏Å‡πå (URLs)",
            "en": "Links (URLs)"
        },
        "general_references": {
            "th": "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ",
            "en": "General References"
        },
        "download_analyze": {
            "th": "‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå",
            "en": "Download and Analyze"
        },
        "download_success": {
            "th": "‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à",
            "en": "Download and analysis successful"
        },
        "loading": {
            "th": "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...",
            "en": "Processing..."
        },
        "graph_title": {
            "th": "‡∏Å‡∏£‡∏≤‡∏ü‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á",
            "en": "Reference Relationship Graph"
        },
        "export_pdf": {
            "th": "‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡πá‡∏ô PDF",
            "en": "Export Graph as PDF"
        },
        "insights_title": {
            "th": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡∏à‡∏≤‡∏Å References",
            "en": "Insights from References"
        },
        "year_distribution": {
            "th": "‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á References ‡∏ï‡∏≤‡∏°‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡∏µ‡∏û‡∏¥‡∏°‡∏û‡πå",
            "en": "References Distribution by Publication Year"
        },
        "publisher_distribution": {
            "th": "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô References ‡∏ï‡∏≤‡∏°‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°‡∏û‡πå",
            "en": "References Count by Publisher"
        },
        "publication_year": {
            "th": "‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡∏µ‡∏û‡∏¥‡∏°‡∏û‡πå",
            "en": "Publication Year"
        },
        "references_count": {
            "th": "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô References",
            "en": "References Count"
        },
        "publisher": {
            "th": "‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°‡∏û‡πå",
            "en": "Publisher"
        },
        "upload_prompt_bibtex": {
            "th": "‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå BibTeX",
            "en": "Or Upload BibTeX File"
        },
        "upload_prompt_ris": {
            "th": "‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå RIS (EndNote)",
            "en": "Or Upload RIS File (EndNote)"
        },
        "citation_network": {
            "th": "‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á",
            "en": "Citation Network"
        },
        "author_network": {
            "th": "‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á",
            "en": "Author Network"
        },
        "keyword_analysis": {
            "th": "‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç",
            "en": "Keyword Analysis"
        },
        "top_keywords": {
            "th": "‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏°",
            "en": "Top Keywords"
        },
        "wordcloud": {
            "th": "‡πÄ‡∏°‡∏Ü‡∏Ñ‡∏≥",
            "en": "Word Cloud"
        },
        "compare_documents": {
            "th": "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£",
            "en": "Compare Documents"
        },
        "select_documents": {
            "th": "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö",
            "en": "Select Documents to Compare"
        },
        "common_references": {
            "th": "‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô",
            "en": "Common References"
        },
        "unique_references": {
            "th": "‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô",
            "en": "Unique References"
        },
        "document_meta": {
            "th": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏°‡∏ï‡∏≤‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£",
            "en": "Document Metadata"
        },
        "journal": {
            "th": "‡∏ß‡∏≤‡∏£‡∏™‡∏≤‡∏£",
            "en": "Journal"
        },
        "authors": {
            "th": "‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á",
            "en": "Authors"
        },
        "no_data": {
            "th": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
            "en": "No data found"
        },
        "import_from_doi": {
            "th": "‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏≤‡∏Å DOI",
            "en": "Import from DOI"
        },
        "enter_doi": {
            "th": "‡∏õ‡πâ‡∏≠‡∏ô DOI",
            "en": "Enter DOI"
        },
        "import": {
            "th": "‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤",
            "en": "Import"
        },
        "settings": {
            "th": "‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤",
            "en": "Settings"
        },
        "language_setting": {
            "th": "‡∏†‡∏≤‡∏©‡∏≤",
            "en": "Language"
        },
        "cache_setting": {
            "th": "‡∏•‡πâ‡∏≤‡∏á‡πÅ‡∏Ñ‡∏ä",
            "en": "Clear Cache"
        },
        "saved_references": {
            "th": "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ",
            "en": "Saved References"
        },
        "save_reference": {
            "th": "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ô‡∏µ‡πâ",
            "en": "Save this Reference"
        },
        "edit_reference": {
            "th": "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á",
            "en": "Edit Reference"
        },
        "update": {
            "th": "‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï",
            "en": "Update"
        },
        "batch_download": {
            "th": "‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î",
            "en": "Batch Download All Documents"
        },
        "download_selected": {
            "th": "‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å",
            "en": "Download Selected Documents"
        },
        "references_viz": {
            "th": "‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏†‡∏≤‡∏û",
            "en": "Visual Analysis"
        },
        "references_table": {
            "th": "‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á",
            "en": "References Table"
        },
        "export_csv": {
            "th": "‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô CSV",
            "en": "Export as CSV"
        },
        "export_bibtex": {
            "th": "‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô BibTeX",
            "en": "Export as BibTeX"
        },
        "import_method": {
            "th": "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤",
            "en": "Select Import Method"
        }
    }
    
    if key in texts:
        if st.session_state.language in texts[key]:
            return texts[key][st.session_state.language]
        return texts[key]["en"]  # ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    return key

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô API ‡πÅ‡∏Ñ‡∏ä
def get_cache_path(url_or_doi):
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Ñ‡∏ä
    safe_name = base64.urlsafe_b64encode(url_or_doi.encode()).decode()
    return os.path.join(st.session_state.cache_dir, f"{safe_name}.pdf")

@st.cache_data(ttl=3600)
def cached_download_content(url):
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏Ñ‡∏ä‡∏Å‡πà‡∏≠‡∏ô
    cache_path = get_cache_path(url)
    if os.path.exists(cache_path):
        with open(cache_path, 'rb') as f:
            return f.read(), "‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÅ‡∏Ñ‡∏ä"
    
    # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
    content, message = download_content(url)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÅ‡∏Ñ‡∏ä ‡∏ñ‡πâ‡∏≤‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    if content and "‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à" in message:
        with open(cache_path, 'wb') as f:
            f.write(content)
    
    return content, message

def clear_cache():
    # ‡∏•‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÅ‡∏Ñ‡∏ä‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    for filename in os.listdir(st.session_state.cache_dir):
        file_path = os.path.join(st.session_state.cache_dir, filename)
        try:
            if os.path.isfile(file_path):
                os.unlink(file_path)
        except Exception as e:
            logging.error(f"Error deleting {file_path}: {e}")
    return "‡∏•‡πâ‡∏≤‡∏á‡πÅ‡∏Ñ‡∏ä‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß"

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á references ‡∏à‡∏≤‡∏Å PDF (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
def extract_references_from_pdf(pdf_file):
    pdf_reader = PyPDF2.PdfReader(pdf_file)
    text = ""
    total_pages = len(pdf_reader.pages)
    
    progress_bar = st.session_state.progress_bar.progress(0, text=get_ui_text("loading"))
    
    for i, page in enumerate(pdf_reader.pages):
        text += page.extract_text() + "\n"
        progress = (i + 1) / total_pages
        progress_bar.progress(progress, text=f"{get_ui_text('loading')} {i+1}/{total_pages}")
        time.sleep(0.05)  # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
    
    # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
    reference_patterns = [
        r'(?:\[\d+\]\s+.*?(?=\n\[|\n\n|$))',  # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö [1] Author...
        r'(?:\d+\.\s+.*?(?=\n\d+\.|\n\n|$))',  # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö 1. Author...
        r'(?:^[A-Z][a-z]+,\s+[A-Z]\..*?(?=\n[A-Z][a-z]+,|\n\n|$))',  # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö APA
        r'https?://[^\s]+',  # URLs
        r'(?:doi:\s*[^\s]+)'  # DOIs
    ]
    
    all_references = []
    
    for pattern in reference_patterns:
        matches = re.findall(pattern, text, re.MULTILINE | re.DOTALL)
        all_references.extend(matches)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô References/Bibliography
    ref_section_patterns = [
        r'(?:References|Bibliography|REFERENCES|BIBLIOGRAPHY)[\s\n]+([\s\S]+?)(?=\n\s*\n\s*[A-Z]{2,}|\Z)',
        r'(?:‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á|‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á|‡∏ö‡∏£‡∏£‡∏ì‡∏≤‡∏ô‡∏∏‡∏Å‡∏£‡∏°)[\s\n]+([\s\S]+?)(?=\n\s*\n\s*[‡∏Å-‡πô]{2,}|\Z)'
    ]
    
    for pattern in ref_section_patterns:
        match = re.search(pattern, text)
        if match:
            ref_section = match.group(1)
            # ‡πÅ‡∏¢‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
            lines = ref_section.split('\n')
            current_ref = ""
            
            for line in lines:
                if re.match(r'^\s*\[\d+\]|\d+\.|\s*[A-Z][a-z]+,\s+[A-Z]\.', line):
                    if current_ref:
                        all_references.append(current_ref.strip())
                    current_ref = line
                else:
                    current_ref += " " + line
            
            if current_ref:  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
                all_references.append(current_ref.strip())
    
    # ‡∏•‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
    unique_references = []
    for ref in all_references:
        ref = ref.strip()
        if ref and ref not in unique_references:
            unique_references.append(ref)
    
    progress_bar.empty()
    
    return unique_references

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á DOI ‡∏à‡∏≤‡∏Å reference (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
def extract_doi(ref):
    patterns = [
        r'doi:\s*([^\s,;]+)',
        r'https?://doi.org/([^\s,;]+)',
        r'10\.\d{4,}\/[^\s,;)]+' # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö DOI ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
    ]
    for pattern in patterns:
        doi_match = re.search(pattern, ref, re.IGNORECASE)
        if doi_match:
            doi = doi_match.group(1) if 'doi:' in pattern or 'doi.org' in pattern else doi_match.group(0)
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î DOI
            return doi.strip('.,;:()')
    return None

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏õ‡∏µ‡∏à‡∏≤‡∏Å reference (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
def extract_year(ref):
    # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏õ‡∏µ ‡∏Ñ‡∏®. ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
    year_match = re.search(r'\b(19\d{2}|20\d{2})\b', ref)
    if year_match:
        return int(year_match.group(0))
    
    # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏ô‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö (2020)
    year_paren_match = re.search(r'\((\d{4})\)', ref)
    if year_paren_match:
        return int(year_paren_match.group(1))
    
    # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏õ‡∏µ ‡∏û‡∏®.
    be_match = re.search(r'\b(25\d{2})\b', ref)
    if be_match:
        be_year = int(be_match.group(0))
        return be_year - 543  # ‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å ‡∏û.‡∏®. ‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ.‡∏®.
    
    return None

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏£‡∏∞‡∏ö‡∏∏‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°‡∏û‡πå‡∏à‡∏≤‡∏Å DOI (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
def extract_publisher(doi):
    if not doi or not isinstance(doi, str) or not doi.startswith("10"):
        return "Unknown"
    
    try:
        doi_prefix = doi.split("/")[0]
        publisher_map = {
            "10.1109": "IEEE",
            "10.1016": "Elsevier",
            "10.1007": "Springer",
            "10.1021": "American Chemical Society",
            "10.1038": "Nature Publishing Group",
            "10.1002": "Wiley",
            "10.1080": "Taylor & Francis",
            "10.1177": "SAGE Publishing",
            "10.3390": "MDPI",
            "10.1371": "Public Library of Science (PLOS)",
            "10.3389": "Frontiers",
            "10.1093": "Oxford University Press",
            "10.1111": "Blackwell Publishing (Wiley)",
            "10.1145": "Association for Computing Machinery (ACM)",
            "10.1088": "IOP Publishing",
            "10.1063": "AIP Publishing",
            "10.1073": "National Academy of Sciences (PNAS)",
            "10.4236": "Scientific Research Publishing",
            "10.1017": "Cambridge University Press",
            "10.5281": "Zenodo",
            "10.5334": "Ubiquity Press",
            "10.3233": "IOS Press",
            "10.5772": "IntechOpen",
            "10.5194": "Copernicus Publications",
            "10.2514": "American Institute of Aeronautics and Astronautics",
            "10.3844": "Science Publications",
            "10.1051": "EDP Sciences",
            "10.4018": "IGI Global",
            "10.1186": "BioMed Central",
            "10.1039": "Royal Society of Chemistry",
            "10.1042": "Portland Press",
            "10.3390": "MDPI"
        }
        return publisher_map.get(doi_prefix, "Other Publisher")
    except:
        return "Unknown"

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏£‡∏™‡∏≤‡∏£‡∏à‡∏≤‡∏Å reference
def extract_journal(ref):
    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏£‡∏™‡∏≤‡∏£
    patterns = [
        r'(?:[,.]\s+)([A-Z][A-Za-z\s&]+)(?:[,.]\s+(?:Vol|Journal|Conference|Proc|\d{1,2}\(\d{1,2}\)))',
        r'"([^"]+)"(?:[,.]\s+(?:Vol|Journal|\d{1,2}\(\d{1,2}\)))',
        r'\'([^\']+)\'(?:[,.]\s+(?:Vol|Journal|\d{1,2}\(\d{1,2}\)))',
        r'In(?::|.\s+)([A-Z][A-Za-z\s&]+)(?:[,.]\s+(?:Vol|Proceedings|Conference|\d{1,2}\(\d{1,2}\)))'
    ]
    
    for pattern in patterns:
        journal_match = re.search(pattern, ref)
        if journal_match:
            return journal_match.group(1).strip()
    
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏Å‡πá‡∏•‡∏≠‡∏á‡πÄ‡∏î‡∏≤‡∏à‡∏≤‡∏Å‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô
    capital_words_match = re.search(r'[,.]\s+([A-Z][A-Z& ]{2,}[a-z]*(?:\s+[A-Z][a-z]+){0,3})[,.]', ref)
    if capital_words_match:
        return capital_words_match.group(1).strip()
    
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ
    return "Unknown Journal"

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á‡∏à‡∏≤‡∏Å reference
def extract_authors(ref):
    # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ
    patterns = [
        # [1] Author1, A., Author2, B. (Year)
        r'^\s*(?:\[\d+\])?\s*([A-Za-z,\.\s]+?)(?:\(|\d{4}|Vol|Journal)',
        # Author1, A., & Author2, B. (Year)
        r'^([A-Za-z,\.\s&]+?)(?:\(|\d{4})',
        # Author1, A. and Author2, B. (Year)
        r'^([A-Za-z,\.\s]+?(?:\sand\s)[A-Za-z,\.\s]+?)(?:\(|\d{4})'
    ]
    
    for pattern in patterns:
        author_match = re.search(pattern, ref)
        if author_match:
            authors = author_match.group(1).strip()
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            authors = re.sub(r'\s+', ' ', authors)
            return authors
    
    return "Unknown Authors"

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•
def extract_all_authors(ref):
    authors_text = extract_authors(ref)
    if authors_text == "Unknown Authors":
        return []
    
    # ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏° patterns ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
    authors = []
    
    # ‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢ "and" ‡∏´‡∏£‡∏∑‡∏≠ "&"
    if " and " in authors_text.lower() or "&" in authors_text:
        parts = re.split(r'\s+and\s+|\s+&\s+', authors_text, flags=re.IGNORECASE)
        for part in parts:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏∏‡∏•‡∏†‡∏≤‡∏Ñ
            if "," in part and not re.search(r'[A-Z]\.,', part):
                commas = part.split(',')
                for i in range(0, len(commas)-1, 2):
                    if i+1 < len(commas):
                        author = f"{commas[i].strip()}, {commas[i+1].strip()}"
                        authors.append(author)
            else:
                authors.append(part.strip())
    else:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ and ‡∏´‡∏£‡∏∑‡∏≠ & ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏∏‡∏•‡∏†‡∏≤‡∏Ñ
        parts = authors_text.split(',')
        
        i = 0
        while i < len(parts) - 1:
            if re.match(r'^\s*[A-Z]\.', parts[i+1]):
                # ‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏ç‡πà‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏à‡∏∏‡∏î ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡πà‡∏≠
                author = f"{parts[i].strip()}, {parts[i+1].strip()}"
                if i+2 < len(parts) and re.match(r'^\s*[A-Z]\.', parts[i+2]):
                    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏•‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢
                    author += f", {parts[i+2].strip()}"
                    i += 3
                else:
                    i += 2
                authors.append(author)
            else:
                # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ
                i += 1
    
    # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á (‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 2 ‡∏ï‡∏±‡∏ß)
    return [author for author in authors if len(author.strip()) > 2]

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å reference
def extract_title(ref):
    # ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏µ‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô
    author_year_pattern = r'^(?:\[\d+\]\s*)?(?:[A-Za-z,\.\s&]+?)(?:\(\d{4}\)|\d{4})\s*(.+?)(?:[,\.]\s+(?:In:|Vol|Journal|pp\.|pages|doi:)|$)'
    title_match = re.search(author_year_pattern, ref)
    
    if title_match:
        title = title_match.group(1).strip()
        # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏≠‡∏±‡∏ç‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏≠‡∏≠‡∏Å
        title = title.strip('"\'')
        # ‡∏•‡∏ö‡∏à‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        title = title.rstrip('.')
        return title
    
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏£‡∏Å ‡∏•‡∏≠‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2
    year_match = re.search(r'\b(19\d{2}|20\d{2})\b', ref)
    if year_match:
        year_pos = year_match.start()
        after_year = ref[year_pos+4:].strip()
        # ‡∏´‡∏≤‡∏à‡∏∏‡∏î‡πÅ‡∏£‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏∏‡∏•‡∏†‡∏≤‡∏Ñ
        end_pos = after_year.find('.')
        if end_pos == -1:
            end_pos = after_year.find(',')
        if end_pos != -1:
            title = after_year[:end_pos].strip()
            return title
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ 10 ‡∏Ñ‡∏≥‡πÅ‡∏£‡∏Å
            words = after_year.split()
            title = ' '.join(words[:min(10, len(words))])
            return title + "..."
    
    return "Unknown Title"

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
def analyze_keyword_frequency(references, top_n=20):
    common_words = set([
        'the', 'and', 'of', 'in', 'on', 'for', 'with', 'to', 'a', 'an', 'by', 'is', 'are', 'was', 'were', 
        'that', 'this', 'these', 'those', 'it', 'as', 'at', 'be', 'from', 'has', 'have', 'had', 
        'doi', 'vol', 'volume', 'issue', 'pp', 'page', 'pages', 'journal', 'conference', 'proceedings',
        'international', 'research', 'university', 'year', 'new', 'study', 'analysis', 'author', 'authors',
        'published', 'publication', 'article', 'paper', 'press'
    ])
    
    word_count = {}
    for ref in references:
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å
        ref_lower = ref.lower()
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
        words = re.findall(r'\b[a-z]{3,}\b', ref_lower)
        for word in words:
            if word not in common_words:
                word_count[word] = word_count.get(word, 0) + 1
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢ ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å top_n
    return sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:top_n]

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á
def analyze_author_network(references):
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô
    author_network = nx.Graph()
    
    for ref in references:
        authors = extract_all_authors(ref)
        if len(authors) > 1:
            for i in range(len(authors)):
                if not author_network.has_node(authors[i]):
                    author_network.add_node(authors[i], count=1)
                else:
                    author_network.nodes[authors[i]]['count'] += 1
                    
                for j in range(i+1, len(authors)):
                    if author_network.has_edge(authors[i], authors[j]):
                        author_network[authors[i]][authors[j]]['weight'] += 1
                    else:
                        author_network.add_edge(authors[i], authors[j], weight=1)
    
    return author_network

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö pyvis
def create_author_network_graph(author_network):
    if not author_network or author_network.number_of_nodes() == 0:
        return None
    
    net = Network(height="500px", width="100%", notebook=False)
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÇ‡∏´‡∏ô‡∏î‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏á‡∏≤‡∏ô
    max_count = max([data.get('count', 1) for _, data in author_network.nodes(data=True)])
    min_size = 10
    max_size = 30
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏´‡∏ô‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î
    for node, data in author_network.nodes(data=True):
        count = data.get('count', 1)
        size = min_size + (max_size - min_size) * (count / max_count)
        net.add_node(node, title=f"{node} ({count} publications)", size=size, color="#3498db")
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏™‡πâ‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°
    for u, v, data in author_network.edges(data=True):
        weight = data.get('weight', 1)
        net.add_edge(u, v, title=f"Collaborated on {weight} papers", width=weight)
    
    # ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
    net.set_options("""
    var options = {
      "nodes": {
        "shape": "dot",
        "font": {
          "size": 12,
          "face": "Tahoma"
        }
      },
      "edges": {
        "color": {
          "inherit": true
        },
        "smooth": {
          "enabled": false
        }
      },
      "physics": {
        "forceAtlas2Based": {
          "gravitationalConstant": -50,
          "centralGravity": 0.01,
          "springLength": 100,
          "springConstant": 0.08
        },
        "maxVelocity": 50,
        "solver": "forceAtlas2Based",
        "timestep": 0.35,
        "stabilization": {
          "enabled": true,
          "iterations": 1000
        }
      }
    }
    """)
    
    net.save_graph("author_network.html")
    with open("author_network.html", "r", encoding="utf-8") as f:
        graph_html = f.read()
    
    return graph_html

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå PDF ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ HTML
def extract_pdf_link(html_content, base_url):
    soup = BeautifulSoup(html_content, "html.parser")
    
    # ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà 1: ‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏¢‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå .pdf
    pdf_links = soup.find_all("a", href=re.compile(r'\.pdf(\?.*)?'))
    
    # ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà 2: ‡∏´‡∏≤‡∏ï‡∏≤‡∏° mime type ‡∏´‡∏£‡∏∑‡∏≠ class ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
    if not pdf_links:
        pdf_links = soup.find_all("a", attrs={"type": "application/pdf"})
    
    # ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà 3: ‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
    if not pdf_links:
        pdf_links = soup.find_all("a", text=re.compile(r'PDF|Full Text|Download', re.IGNORECASE))
    
    # ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà 4: ‡∏´‡∏≤‡∏à‡∏≤‡∏Å class ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå PDF
    if not pdf_links:
        pdf_links = soup.find_all("a", class_=re.compile(r'pdf|full[-_]?text|download', re.IGNORECASE))
    
    for link in pdf_links:
        href = link.get("href")
        if href:
            if href.startswith("http"):
                return href
            else:
                # ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô relative URL ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö base_url
                return urljoin(base_url, href)
    
    return None

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CrossRef API
def query_crossref_api(doi):
    if not doi:
        return None
    
    url = f"https://api.crossref.org/works/{doi}"
    headers = {
        "User-Agent": "ReferenceAnalyzer/1.0 (mailto:user@example.com)",
        "Accept": "application/json"
    }
    
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            data = response.json()
            return data['message']
        else:
            return None
    except Exception as e:
        logging.error(f"Error accessing Crossref API: {str(e)}")
        return None

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
def download_content(url):
    progress_bar = st.session_state.progress_bar.progress(0, text=get_ui_text("loading"))
    
    try:
        if not url.startswith("http"):
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô DOI ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° URL ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤
            if url.startswith("10."):
                url = f"https://doi.org/{url}"
            else:
                return None, "URL ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö DOI ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏¥‡∏á‡∏Å‡πå"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml,application/pdf;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        
        progress_bar.progress(0.2, text=f"{get_ui_text('loading')} - ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠")
        
        response = requests.get(url, headers=headers, timeout=20, allow_redirects=True)
        
        progress_bar.progress(0.5, text=f"{get_ui_text('loading')} - ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á")
        
        if response.status_code == 200:
            content_type = response.headers.get("content-type", "").lower()
            
            if "application/pdf" in content_type:
                progress_bar.progress(1.0, text=f"{get_ui_text('loading')} - ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                return response.content, "‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î PDF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à"
            
            elif "text/html" in content_type:
                progress_bar.progress(0.6, text=f"{get_ui_text('loading')} - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå PDF")
                
                # ‡πÉ‡∏ä‡πâ CrossRef API ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô DOI
                if "doi.org" in url:
                    doi = url.split("doi.org/")[1]
                    metadata = query_crossref_api(doi)
                    if metadata and "link" in metadata:
                        for link in metadata.get("link", []):
                            if link.get("content-type") == "application/pdf":
                                pdf_url = link.get("URL")
                                if pdf_url:
                                    pdf_response = requests.get(pdf_url, headers=headers, timeout=20)
                                    if pdf_response.status_code == 200 and "application/pdf" in pdf_response.headers.get("content-type", "").lower():
                                        progress_bar.progress(1.0, text=f"{get_ui_text('loading')} - ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î PDF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏ú‡πà‡∏≤‡∏ô CrossRef)")
                                        return pdf_response.content, "‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î PDF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏à‡∏≤‡∏Å CrossRef API"
                
                # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå PDF ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö
                progress_bar.progress(0.7, text=f"{get_ui_text('loading')} - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö")
                pdf_url = extract_pdf_link(response.text, url)
                
                if pdf_url:
                    progress_bar.progress(0.8, text=f"{get_ui_text('loading')} - ‡∏û‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå PDF ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î")
                    # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î PDF ‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö
                    pdf_response = requests.get(pdf_url, headers=headers, timeout=20)
                    if pdf_response.status_code == 200 and "application/pdf" in pdf_response.headers.get("content-type", "").lower():
                        progress_bar.progress(1.0, text=f"{get_ui_text('loading')} - ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î PDF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                        return pdf_response.content, "‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î PDF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö"
                    elif pdf_response.status_code == 200:
                        # ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ content-type ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                        if pdf_response.content.startswith(b'%PDF'):
                            progress_bar.progress(1.0, text=f"{get_ui_text('loading')} - ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î PDF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                            return pdf_response.content, "‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î PDF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö"
                        else:
                            return None, "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô PDF"
                    else:
                        return None, f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î PDF ‡πÑ‡∏î‡πâ (‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: {pdf_response.status_code})"
                else:
                    return None, "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå PDF ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö"
            
            else:
                return None, f"‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: {content_type}"
        
        elif response.status_code == 404:
            return None, "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£ (404): ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏≠‡∏≤‡∏à‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡∏π‡∏Å‡∏•‡∏ö"
        
        elif response.status_code == 403:
            return None, "‡∏ñ‡∏π‡∏Å‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á (403): ‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ VPN"
        
        else:
            return None, f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏î‡πâ (‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: {response.status_code})"
    
    except requests.exceptions.RequestException as e:
        logging.error(f"Error downloading content: {str(e)}")
        return None, f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠: {str(e)}"
    
    finally:
        progress_bar.empty()

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ BibTeX
def import_bibtex(bibtex_file):
    text = bibtex_file.read().decode('utf-8')
    references = []
    
    try:
        bib_database = bibtexparser.loads(text)
        
        for entry in bib_database.entries:
            authors = entry.get('author', 'Unknown Authors')
            title = entry.get('title', 'Untitled')
            year = entry.get('year', '')
            journal = entry.get('journal', entry.get('booktitle', 'Unknown Source'))
            volume = entry.get('volume', '')
            number = entry.get('number', '')
            pages = entry.get('pages', '')
            doi = entry.get('doi', '')
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
            ref = f"{authors}. ({year}). {title}. {journal}"
            
            if volume:
                ref += f", Vol. {volume}"
            if number:
                ref += f"({number})"
            if pages:
                ref += f", pp. {pages}"
            if doi:
                ref += f". doi: {doi}"
            
            references.append(ref)
        
        return references
    
    except Exception as e:
        logging.error(f"Error parsing BibTeX: {str(e)}")
        st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå BibTeX: {str(e)}")
        return []

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ RIS (EndNote)
def import_ris(ris_file):
    text = ris_file.read().decode('utf-8')
    references = []
    
    # ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
    entries = text.split('\nER  - \n')
    
    for entry in entries:
        if not entry.strip():
            continue
        
        lines = entry.strip().split('\n')
        entry_data = {}
        
        for line in lines:
            if not line or '  - ' not in line:
                continue
            
            tag, value = line.split('  - ', 1)
            tag = tag.strip()
            value = value.strip()
            
            if tag in entry_data:
                if isinstance(entry_data[tag], list):
                    entry_data[tag].append(value)
                else:
                    entry_data[tag] = [entry_data[tag], value]
            else:
                entry_data[tag] = value
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á
        authors = []
        for tag in ['AU', 'A1', 'A2', 'A3', 'A4']:
            if tag in entry_data:
                if isinstance(entry_data[tag], list):
                    authors.extend(entry_data[tag])
                else:
                    authors.append(entry_data[tag])
        
        authors_str = ', '.join(authors) if authors else 'Unknown Authors'
        
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏∑‡πà‡∏ô‡πÜ
        title = entry_data.get('TI', entry_data.get('T1', 'Untitled'))
        year = entry_data.get('PY', entry_data.get('Y1', ''))
        if year and len(year) >= 4:
            year = year[:4]
        
        journal = entry_data.get('JO', entry_data.get('T2', entry_data.get('JA', 'Unknown Source')))
        volume = entry_data.get('VL', '')
        issue = entry_data.get('IS', '')
        pages = entry_data.get('SP', '')
        
        if 'EP' in entry_data and pages:
            pages = f"{pages}-{entry_data['EP']}"
        
        doi = entry_data.get('DO', '')
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
        ref = f"{authors_str}. ({year}). {title}. {journal}"
        
        if volume:
            ref += f", Vol. {volume}"
        if issue:
            ref += f"({issue})"
        if pages:
            ref += f", pp. {pages}"
        if doi:
            ref += f". doi: {doi}"
        
        references.append(ref)
    
    return references

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô BibTeX
def export_to_bibtex(references):
    bibtex_output = ""
    
    for i, ref in enumerate(references):
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á entry key
        key = f"ref{i+1}"
        
        # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å reference
        authors = extract_authors(ref)
        title = extract_title(ref)
        year = extract_year(ref)
        journal = extract_journal(ref)
        doi = extract_doi(ref)
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        if "Unknown Authors" not in authors:
            authors = authors.replace(" and ", " AND ")
        
        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á entry
        bibtex_output += f"@article{{{key},\n"
        bibtex_output += f"  author = {{{authors}}},\n"
        bibtex_output += f"  title = {{{title}}},\n"
        
        if year:
            bibtex_output += f"  year = {{{year}}},\n"
        
        if "Unknown Journal" not in journal:
            bibtex_output += f"  journal = {{{journal}}},\n"
        
        if doi:
            bibtex_output += f"  doi = {{{doi}}},\n"
        
        # ‡∏õ‡∏¥‡∏î entry
        bibtex_output += "}\n\n"
    
    return bibtex_output

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏ö‡∏ö star
def create_reference_graph(central_node, all_references, doi_references):
    progress_bar = st.session_state.progress_bar.progress(0, text=f"{get_ui_text('loading')} - ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü")
    
    G = nx.DiGraph()
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏ô‡∏î (‡∏ï‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á‡∏ñ‡πâ‡∏≤‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)
    central_label = central_node[:50] + ("..." if len(central_node) > 50 else "")
    G.add_node(central_label, color="red", size=30, title=central_node)
    
    total_refs = len(all_references)
    for i, ref in enumerate(all_references):
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏µ
        if "doi:" in ref.lower() or ref.lower().startswith("http"):
            color = "skyblue"
        else:
            color = "gray"
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏ô‡∏î
        label = ref[:50] + ("..." if len(ref) > 50 else "")
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏´‡∏ô‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡πâ‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°
        G.add_node(label, color=color, size=20, title=ref)
        G.add_edge(central_label, label)
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏ñ‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤
        progress = (i + 1) / total_refs
        progress_bar.progress(progress, text=f"{get_ui_text('loading')} - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏´‡∏ô‡∏î {i+1}/{total_refs}")
        if i % 10 == 0:  # ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï UI
            time.sleep(0.01)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏ö‡∏ö pyvis
    net = Network(height="500px", width="100%", directed=True, notebook=False)
    net.from_nx(G)
    
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
    net.set_options("""
    var options = {
      "nodes": {
        "shape": "dot",
        "font": {
          "size": 12,
          "face": "tahoma"
        },
        "color": {
          "highlight": "#FF0000",
          "hover": "#FF9999"
        }
      },
      "edges": {
        "arrows": "to",
        "smooth": {
          "type": "continuous",
          "forceDirection": "none",
          "roundness": 0.2
        }
      },
      "physics": {
        "hierarchicalRepulsion": {
          "centralGravity": 0.0,
          "springLength": 150,
          "springConstant": 0.01,
          "nodeDistance": 120,
          "damping": 0.09
        },
        "solver": "hierarchicalRepulsion"
      },
      "interaction": {
        "navigationButtons": true,
        "keyboard": true
      }
    }
    """)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏£‡∏≤‡∏ü
    net.save_graph("graph.html")
    with open("graph.html", "r", encoding="utf-8") as f:
        graph_html = f.read()
    
    progress_bar.progress(1.0, text=f"{get_ui_text('loading')} - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    time.sleep(0.1)
    progress_bar.empty()
    
    return graph_html

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡πá‡∏ô PDF
def export_graph_to_pdf(central_node, all_references):
    progress_bar = st.session_state.progress_bar.progress(0, text=f"{get_ui_text('loading')} - ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡πá‡∏ô PDF")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü NetworkX
    G = nx.DiGraph()
    
    central_label = central_node[:30] + ("..." if len(central_node) > 30 else "")
    G.add_node(central_label, color="red")
    
    ref_nodes = []
    for ref in all_references:
        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÇ‡∏´‡∏ô‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å PDF ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°
        if len(ref_nodes) >= 20:
            ref_nodes.append("...")
            break
            
        color = "skyblue" if "doi:" in ref.lower() or ref.lower().startswith("http") else "gray"
        label = ref[:30] + ("..." if len(ref) > 30 else "")
        G.add_node(label, color=color)
        G.add_edge(central_label, label)
        ref_nodes.append(label)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á PDF
    plt.figure(figsize=(10, 8))
    pos = nx.spring_layout(G, k=0.3, iterations=50)
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏µ
    node_colors = [G.nodes[n].get('color', 'gray') for n in G.nodes()]
    
    # ‡∏ß‡∏≤‡∏î‡πÇ‡∏´‡∏ô‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡πâ‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°
    nx.draw(G, pos, with_labels=True, node_color=node_colors, 
            node_size=500, font_size=8, arrows=True, 
            arrowsize=15, width=1.5, alpha=0.8,
            font_weight='bold', font_family='sans-serif')
    
    plt.title(f"{get_ui_text('graph_title')} ({central_label})")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô PDF
    pdf_buffer = BytesIO()
    with PdfPages(pdf_buffer) as pdf:
        pdf.savefig(bbox_inches='tight')
    plt.close()
    
    progress_bar.progress(1.0, text=f"{get_ui_text('loading')} - ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å PDF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    time.sleep(0.1)
    progress_bar.empty()
    
    pdf_buffer.seek(0)
    return pdf_buffer

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á word cloud
def create_wordcloud(references):
    # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    text = " ".join(references)
    
    # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    text = re.sub(r'\b\d+\b', '', text)  # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
    text = re.sub(r'http\S+|www\.\S+', '', text)  # ‡∏•‡∏ö URL
    text = re.sub(r'doi:\s*\S+', '', text)  # ‡∏•‡∏ö DOI
    
    # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å
    text = re.sub(r'[^\w\s]', '', text).lower()
    
    # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    stopwords = set([
        'the', 'and', 'of', 'in', 'on', 'for', 'with', 'to', 'a', 'an', 'by', 'is', 'are', 'was', 'were', 
        'that', 'this', 'these', 'those', 'it', 'as', 'at', 'be', 'from', 'has', 'have', 'had', 
        'doi', 'vol', 'volume', 'issue', 'pp', 'page', 'pages', 'journal', 'conference', 'proceedings',
        'international', 'university', 'year', 'new', 'study', 'analysis'
    ])
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á word cloud
    wordcloud = WordCloud(width=800, height=400, background_color='white',
                         stopwords=stopwords, max_words=100, 
                         collocations=False, colormap='viridis').generate(text)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    img_buffer = BytesIO()
    plt.savefig(img_buffer, format='png', bbox_inches='tight')
    plt.close()
    
    img_buffer.seek(0)
    return img_buffer

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
def compare_documents(doc1_refs, doc2_refs):
    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ã‡∏ï
    ref_set1 = set([ref.lower() for ref in doc1_refs])
    ref_set2 = set([ref.lower() for ref in doc2_refs])
    
    # ‡∏´‡∏≤‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
    common_refs = ref_set1.intersection(ref_set2)
    unique_doc1 = ref_set1 - ref_set2
    unique_doc2 = ref_set2 - ref_set1
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    total_doc1 = len(doc1_refs)
    total_doc2 = len(doc2_refs)
    total_common = len(common_refs)
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ Jaccard similarity
    jaccard = total_common / (total_doc1 + total_doc2 - total_common) if (total_doc1 + total_doc2 - total_common) > 0 else 0
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    result = {
        "common": list(common_refs),
        "unique_doc1": list(unique_doc1),
        "unique_doc2": list(unique_doc2),
        "total_doc1": total_doc1,
        "total_doc2": total_doc2,
        "total_common": total_common,
        "jaccard_similarity": jaccard
    }
    
    return result

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á dataframe ‡∏à‡∏≤‡∏Å references
def create_references_dataframe(references):
    data = []
    
    for ref in references:
        data.append({
            "Reference": ref,
            "Authors": extract_authors(ref),
            "Title": extract_title(ref),
            "Year": extract_year(ref),
            "Journal": extract_journal(ref),
            "DOI": extract_doi(ref),
            "Publisher": extract_publisher(extract_doi(ref))
        })
    
    return pd.DataFrame(data)

# ‡∏≠‡∏¥‡∏ô‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏ü‡∏ã Streamlit
st.title(get_ui_text("title"))

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ó‡πá‡∏ö
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "üìÑ " + get_ui_text("references_list"), 
    "üìä " + get_ui_text("references_viz"), 
    "üë• " + get_ui_text("author_network"), 
    "üîÑ " + get_ui_text("compare_documents"),
    "‚öôÔ∏è " + get_ui_text("settings")
])

# ‡πÅ‡∏ó‡πá‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
with tab5:
    st.header(get_ui_text("settings"))
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏†‡∏≤‡∏©‡∏≤
    language = st.selectbox(
        get_ui_text("language_setting"),
        options=["th", "en"],
        format_func=lambda x: "‡πÑ‡∏ó‡∏¢" if x == "th" else "English",
        index=0 if st.session_state.language == "th" else 1
    )
    
    if language != st.session_state.language:
        st.session_state.language = language
        st.experimental_rerun()
    
    # ‡∏õ‡∏∏‡πà‡∏°‡∏•‡πâ‡∏≤‡∏á‡πÅ‡∏Ñ‡∏ä
    if st.button(get_ui_text("cache_setting")):
        result = clear_cache()
        st.success(result)
    
    # ‡∏õ‡∏∏‡πà‡∏° Clear PDF
    if st.button(get_ui_text("clear_button")):
        st.session_state.current_doc = None
        st.session_state.references = []
        st.session_state.doi_references = []
        st.session_state.graph_html = None
        st.session_state.doc_history = {}
        st.session_state.author_network = None
        st.session_state.keyword_data = None
        st.success("‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")

# ‡πÅ‡∏ó‡πá‡∏ö 1: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
with tab1:
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤
    import_method = st.radio(
        get_ui_text("import_method"),
        options=["pdf", "bibtex", "ris", "doi"],
        format_func=lambda x: "PDF" if x == "pdf" else 
                         "BibTeX" if x == "bibtex" else
                         "RIS (EndNote)" if x == "ris" else "DOI",
        horizontal=True
    )
    
    if import_method == "pdf":
        uploaded_file = st.file_uploader(get_ui_text("upload_prompt"), type="pdf")
        
        # ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡πÉ‡∏´‡∏°‡πà
        if uploaded_file is not None:
            st.session_state.progress_bar = st.empty()
            progress_bar = st.session_state.progress_bar.progress(0, text=f"{get_ui_text('loading')} - ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• PDF")
            
            time.sleep(0.5)  # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
            references = extract_references_from_pdf(uploaded_file)
            
            if references:
                st.session_state.current_doc = uploaded_file.name
                st.session_state.references = references
                st.session_state.doi_references = [ref for ref in references if "doi:" in ref.lower() or ref.lower().startswith("http")]
                st.session_state.graph_html = create_reference_graph(uploaded_file.name, st.session_state.references, st.session_state.doi_references)
                st.session_state.doc_history[uploaded_file.name] = {
                    "references": references,
                    "doi_references": st.session_state.doi_references
                }
                
                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á
                st.session_state.author_network = analyze_author_network(references)
                
                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                st.session_state.keyword_data = analyze_keyword_frequency(references)
                
                st.success(f"‡∏û‡∏ö {len(references)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á")
            else:
                st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå")
            
            st.session_state.progress_bar.empty()
    
    elif import_method == "bibtex":
        uploaded_bibtex = st.file_uploader(get_ui_text("upload_prompt_bibtex"), type=["bib", "bibtex", "txt"])
        
        if uploaded_bibtex is not None:
            st.session_state.progress_bar = st.empty()
            progress_bar = st.session_state.progress_bar.progress(0, text=f"{get_ui_text('loading')} - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå BibTeX")
            
            references = import_bibtex(uploaded_bibtex)
            
            if references:
                st.session_state.current_doc = uploaded_bibtex.name
                st.session_state.references = references
                st.session_state.doi_references = [ref for ref in references if "doi:" in ref.lower() or ref.lower().startswith("http")]
                st.session_state.graph_html = create_reference_graph(uploaded_bibtex.name, st.session_state.references, st.session_state.doi_references)
                st.session_state.doc_history[uploaded_bibtex.name] = {
                    "references": references,
                    "doi_references": st.session_state.doi_references
                }
                
                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á
                st.session_state.author_network = analyze_author_network(references)
                
                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                st.session_state.keyword_data = analyze_keyword_frequency(references)
                
                st.success(f"‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ {len(references)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å BibTeX ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            else:
                st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå BibTeX ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
            
            st.session_state.progress_bar.empty()
    
    elif import_method == "ris":
        uploaded_ris = st.file_uploader(get_ui_text("upload_prompt_ris"), type=["ris", "txt"])
        
        if uploaded_ris is not None:
            st.session_state.progress_bar = st.empty()
            progress_bar = st.session_state.progress_bar.progress(0, text=f"{get_ui_text('loading')} - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå RIS")
            
            references = import_ris(uploaded_ris)
            
            if references:
                st.session_state.current_doc = uploaded_ris.name
                st.session_state.references = references
                st.session_state.doi_references = [ref for ref in references if "doi:" in ref.lower() or ref.lower().startswith("http")]
                st.session_state.graph_html = create_reference_graph(uploaded_ris.name, st.session_state.references, st.session_state.doi_references)
                st.session_state.doc_history[uploaded_ris.name] = {
                    "references": references,
                    "doi_references": st.session_state.doi_references
                }
                
                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á
                st.session_state.author_network = analyze_author_network(references)
                
                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                st.session_state.keyword_data = analyze_keyword_frequency(references)
                
                st.success(f"‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ {len(references)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å RIS ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            else:
                st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå RIS ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
            
            st.session_state.progress_bar.empty()
            
    elif import_method == "doi":
        doi_input = st.text_input(get_ui_text("enter_doi"), value="10.")
        if st.button(get_ui_text("import")):
            if doi_input‡πÅ‡∏•‡∏∞doi_input.startswith("10."):
                st.session_state.progress_bar = st.empty()
                
                # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å DOI
                url = f"https://doi.org/{doi_input}"
                pdf_content, message = cached_download_content(url)
                
                if pdf_content:
                    pdf_file = BytesIO(pdf_content)
                    references = extract_references_from_pdf(pdf_file)
                    
                    if references:
                        st.session_state.current_doc = doi_input
                        st.session_state.references = references
                        st.session_state.doi_references = [ref for ref in references if "doi:" in ref.lower() or ref.lower().startswith("http")]
                        st.session_state.graph_html = create_reference_graph(doi_input, st.session_state.references, st.session_state.doi_references)
                        st.session_state.doc_history[doi_input] = {
                            "references": references,
                            "doi_references": st.session_state.doi_references
                        }
                        
                        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á
                        st.session_state.author_network = analyze_author_network(references)
                        
                        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                        st.session_state.keyword_data = analyze_keyword_frequency(references)
                        
                        st.success(f"‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå DOI ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏û‡∏ö {len(references)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á")
                    else:
                        st.warning("‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á")
                else:
                    st.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏à‡∏≤‡∏Å DOI: {message}")
                
                st.session_state.progress_bar.empty()
            else:
                st.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏õ‡πâ‡∏≠‡∏ô DOI ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏Ñ‡∏ß‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ 10.")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• references
    if st.session_state.references:
        st.subheader(get_ui_text("references_list"))
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á
        df_refs = create_references_dataframe(st.session_state.references)
        
        # ‡∏õ‡∏∏‡πà‡∏°‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å
        col1, col2 = st.columns(2)
        with col1:
            if st.button(get_ui_text("export_csv")):
                csv = df_refs.to_csv(index=False)
                st.download_button(
                    label="Download CSV",
                    data=csv,
                    file_name=f"references_{st.session_state.current_doc[:20]}.csv",
                    mime="text/csv"
                )
        
        with col2:
            if st.button(get_ui_text("export_bibtex")):
                bibtex = export_to_bibtex(st.session_state.references)
                st.download_button(
                    label="Download BibTeX",
                    data=bibtex,
                    file_name=f"references_{st.session_state.current_doc[:20]}.bib",
                    mime="text/plain"
                )
        
        # ‡∏Å‡∏•‡∏∏‡πà‡∏° 1: ‡πÅ‡∏™‡∏î‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå (URL) ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
        url_references = [ref for ref in st.session_state.references if ref.lower().startswith("http")]
        if url_references:
            st.subheader(get_ui_text("urls_section"))
            for i, ref in enumerate(url_references):
                with st.expander(f"URL {i+1}: {ref[:100]}..."):
                    st.write(f"Full: {ref}")
                    color = "green"
                    st.markdown(f'<p style="color:{color}">{ref}</p>', unsafe_allow_html=True)
                    ref_url = ref
                    if st.button(f"{get_ui_text('download_analyze')} - URL {i+1}", key=f"download_url_{i}"):
                        pdf_content, content = cached_download_content(ref_url)
                        if pdf_content:
                            pdf_file = BytesIO(pdf_content)
                            new_references = extract_references_from_pdf(pdf_file)
                            # ‡πÅ‡∏™‡∏î‡∏á references ‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
                            st.subheader(f"References from {ref_url}")
                            for j, new_ref in enumerate(new_references):
                                with st.expander(f"Sub-reference {j+1}: {new_ref[:100]}..."):
                                    st.write(f"Full: {new_ref}")
                                    color = "green" if "doi:" in new_ref.lower() or new_ref.lower().startswith("http") else "gray"
                                    st.markdown(f'<p style="color:{color}">{new_ref}</p>', unsafe_allow_html=True)
                            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
                            st.session_state.current_doc = ref_url
                            st.session_state.references = new_references
                            st.session_state.doi_references = [r for r in new_references if "doi:" in r.lower() or r.lower().startswith("http")]
                            st.session_state.graph_html = create_reference_graph(ref_url, st.session_state.references, st.session_state.doi_references)
                            st.session_state.doc_history[ref_url] = {
                                "references": new_references,
                                "doi_references": st.session_state.doi_references
                            }
                            st.success(get_ui_text("download_success"))
                        else:
                            st.error(content)
        
        # ‡∏Å‡∏•‡∏∏‡πà‡∏° 2: ‡πÅ‡∏™‡∏î‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (‡∏£‡∏ß‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ DOI) ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
        other_references = [ref for ref in st.session_state.references if not ref.lower().startswith("http")]
        if other_references:
            st.subheader(get_ui_text("general_references"))
            for i, ref in enumerate(other_references):
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
                doi = extract_doi(ref)
                authors = extract_authors(ref)
                year = extract_year(ref)
                title = extract_title(ref)
                journal = extract_journal(ref)
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡∏Å‡∏ß‡πà‡∏≤
                header_text = f"{authors[:30]}{'...' if len(authors) > 30 else ''} ({year if year else 'n/a'})"
                
                with st.expander(f"Ref {i+1}: {header_text}"):
                    st.markdown(f"**Full Reference:** {ref}")
                    st.markdown(f"**Authors:** {authors}")
                    st.markdown(f"**Title:** {title}")
                    st.markdown(f"**Year:** {year if year else 'Not found'}")
                    st.markdown(f"**Journal:** {journal}")
                    
                    if doi:
                        st.markdown(f"**DOI:** [{doi}](https://doi.org/{doi})")
                        st.markdown(f"**Publisher:** {extract_publisher(doi)}")
                        
                        ref_url = f"https://doi.org/{doi}"
                        if st.button(f"{get_ui_text('download_analyze')} - Ref {i+1}", key=f"download_ref_{i}"):
                            pdf_content, content = cached_download_content(ref_url)
                            if pdf_content:
                                pdf_file = BytesIO(pdf_content)
                                new_references = extract_references_from_pdf(pdf_file)
                                # ‡πÅ‡∏™‡∏î‡∏á references ‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
                                if new_references:
                                    st.subheader(f"References from {ref_url}")
                                    for j, new_ref in enumerate(new_references[:5]):  # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏û‡∏µ‡∏¢‡∏á 5 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏£‡∏Å
                                        st.markdown(f"**{j+1}.** {new_ref[:100]}...")
                                    
                                    if len(new_references) > 5:
                                        st.markdown(f"*...and {len(new_references) - 5} more references*")
                                    
                                    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
                                    st.session_state.current_doc = ref_url
                                    st.session_state.references = new_references
                                    st.session_state.doi_references = [r for r in new_references if "doi:" in r.lower() or r.lower().startswith("http")]
                                    st.session_state.graph_html = create_reference_graph(ref_url, st.session_state.references, st.session_state.doi_references)
                                    st.session_state.doc_history[ref_url] = {
                                        "references": new_references,
                                        "doi_references": st.session_state.doi_references
                                    }
                                    
                                    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á
                                    st.session_state.author_network = analyze_author_network(new_references)
                                    
                                    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                                    st.session_state.keyword_data = analyze_keyword_frequency(new_references)
                                    
                                    st.success(f"{get_ui_text('download_success')} - {len(new_references)} references found")
                                else:
                                    st.warning("‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á")
                            else:
                                st.error(content)
                    else:
                        st.markdown("**DOI:** Not found")
    else:
        st.write("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå")

# ‡πÅ‡∏ó‡πá‡∏ö 2: ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏†‡∏≤‡∏û
with tab2:
    if st.session_state.references:
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü
        st.subheader(f"{get_ui_text('graph_title')} ({st.session_state.current_doc})")
        if st.session_state.graph_html:
            html(st.session_state.graph_html, height=550, width=None)
            
            # ‡∏õ‡∏∏‡πà‡∏°‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å PDF
            pdf_buffer = export_graph_to_pdf(st.session_state.current_doc, st.session_state.references)
            st.download_button(
                label=get_ui_text("export_pdf"),
                data=pdf_buffer,
                file_name=f"graph_{st.session_state.current_doc[:20]}.pdf",
                mime="application/pdf"
            )
        
        # Visualization ‡∏î‡πâ‡∏ß‡∏¢ Altair
        st.subheader(get_ui_text("insights_title"))
        
        # 1. ‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á References ‡∏ï‡∏≤‡∏°‡∏õ‡∏µ
        years = [extract_year(ref) for ref in st.session_state.references if extract_year(ref)]
        if years:
            df_years = pd.DataFrame(years, columns=["Year"])
            chart_years = alt.Chart(df_years).mark_bar().encode(
                x=alt.X("Year:O", title=get_ui_text("publication_year")),
                y=alt.Y("count()", title=get_ui_text("references_count")),
                tooltip=["Year", "count()"]
            ).properties(
                title=get_ui_text("year_distribution"),
                width=600,
                height=400
            ).interactive()
            st.altair_chart(chart_years)
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏õ‡∏µ
            years_array = np.array(years)
            if len(years_array) > 0:
                st.write(f"**‡∏õ‡∏µ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (Mean):** {np.mean(years_array):.2f}")
                st.write(f"**‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:** {pd.Series(years).value_counts().index[0]}")
                st.write(f"**‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏µ:** {min(years)} - {max(years)}")
                
                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏¢‡∏∏‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
                current_year = 2025  # ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                ref_ages = current_year - np.array(years)
                st.write(f"**‡∏≠‡∏≤‡∏¢‡∏∏‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢:** {np.mean(ref_ages):.2f} ‡∏õ‡∏µ")
        else:
            st.write(get_ui_text("no_data"))
        
        # 2. ‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô References ‡∏ï‡∏≤‡∏°‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°‡∏û‡πå
        publishers = []
        for ref in st.session_state.references:
            doi = extract_doi(ref)
            publisher = extract_publisher(doi) if doi else "Unknown"
            publishers.append(publisher)
        df_publishers = pd.DataFrame(publishers, columns=["Publisher"])
        if not df_publishers.empty:
            # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢
            publisher_counts = df_publishers['Publisher'].value_counts()
            top_publishers = publisher_counts[publisher_counts >= 2].index.tolist()
            
            # ‡∏£‡∏ß‡∏°‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏õ‡πá‡∏ô "Others"
            df_publishers['Publisher'] = df_publishers['Publisher'].apply(
                lambda x: x if x in top_publishers else "Others")
            
            chart_publishers = alt.Chart(df_publishers).mark_bar().encode(
                x=alt.X("count()", title=get_ui_text("references_count")),
                y=alt.Y("Publisher", title=get_ui_text("publisher"), sort='-x'),
                color="Publisher",
                tooltip=["Publisher", "count()"]
            ).properties(
                title=get_ui_text("publisher_distribution"),
                width=600,
                height=400
            ).interactive()
            st.altair_chart(chart_publishers)
        else:
            st.write(get_ui_text("no_data"))
        
        # 3. Word Cloud ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        st.subheader(get_ui_text("wordcloud"))
        wordcloud_img = create_wordcloud(st.session_state.references)
        st.image(wordcloud_img)
        
        # 4. ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢
        if st.session_state.keyword_data:
            st.subheader(get_ui_text("top_keywords"))
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            df_keywords = pd.DataFrame(st.session_state.keyword_data, columns=["Keyword", "Count"])
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü
            chart_keywords = alt.Chart(df_keywords.head(15)).mark_bar().encode(
                x=alt.X("Count:Q", title="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö"),
                y=alt.Y("Keyword:N", title="‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç", sort='-x'),
                color=alt.Color("Count:Q", scale=alt.Scale(scheme='blues')),
                tooltip=["Keyword", "Count"]
            ).properties(
                title="‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏° 15 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å",
                width=600,
                height=400
            ).interactive()
            st.altair_chart(chart_keywords)
    else:
        st.write("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå")

# ‡πÅ‡∏ó‡πá‡∏ö 3: ‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á
with tab3:
    if st.session_state.author_network and st.session_state.author_network.number_of_nodes() > 0:
        st.subheader(get_ui_text("author_network"))
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á
        author_graph_html = create_author_network_graph(st.session_state.author_network)
        
        if author_graph_html:
            html(author_graph_html, height=550, width=None)
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á
            st.subheader("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á")
            
            # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            total_authors = st.session_state.author_network.number_of_nodes()
            st.write(f"**‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:** {total_authors}")
            
            # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠
            total_collaborations = st.session_state.author_network.number_of_edges()
            st.write(f"**‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:** {total_collaborations}")
            
            # ‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
            if total_authors > 0:
                top_collaborators = sorted(st.session_state.author_network.degree, key=lambda x: x[1], reverse=True)
                
                if top_collaborators:
                    st.write("### ‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Top Collaborators)")
                    top_5 = top_collaborators[:min(5, len(top_collaborators))]
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame
                    df_top_authors = pd.DataFrame([
                        {"‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á": author, "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠": degree}
                        for author, degree in top_5
                    ])
                    
                    st.dataframe(df_top_authors)
        else:
            st.write("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á‡πÑ‡∏î‡πâ")
    else:
        st.write("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡πÅ‡∏ï‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢")

# ‡πÅ‡∏ó‡πá‡∏ö 4: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
with tab4:
    st.subheader(get_ui_text("compare_documents"))
    
    if len(st.session_state.doc_history) >= 2:
        doc_names = list(st.session_state.doc_history.keys())
        
        col1, col2 = st.columns(2)
        
        with col1:
            doc1 = st.selectbox("‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà 1", options=doc_names, index=0)
        
        with col2:
            doc2 = st.selectbox("‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà 2", options=doc_names, index=min(1, len(doc_names)-1))
        
        if doc1 != doc2:
            doc1_refs = st.session_state.doc_history[doc1]["references"]
            doc2_refs = st.session_state.doc_history[doc2]["references"]
            
            if st.button("‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"):
                st.session_state.progress_bar = st.empty()
                progress_bar = st.session_state.progress_bar.progress(0, text="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£...")
                
                # ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
                comparison_result = compare_documents(doc1_refs, doc2_refs)
                
                progress_bar.progress(1.0, text="‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
                time.sleep(0.5)
                st.session_state.progress_bar.empty()
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                st.subheader("‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á
                sim_percent = comparison_result["jaccard_similarity"] * 100
                st.metric("‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (Jaccard Similarity)", f"{sim_percent:.2f}%")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà 1", comparison_result["total_doc1"])
                with col2:
                    st.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà 2", comparison_result["total_doc2"])
                with col3:
                    st.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô", comparison_result["total_common"])
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô
                if comparison_result["common"]:
                    with st.expander(f"{get_ui_text('common_references')} ({len(comparison_result['common'])})", expanded=True):
                        for i, ref in enumerate(comparison_result["common"]):
                            st.markdown(f"**{i+1}.** {ref}")
                else:
                    st.info("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà 1
                if comparison_result["unique_doc1"]:
                    with st.expander(f"‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà 1: {doc1} ({len(comparison_result['unique_doc1'])})", expanded=False):
                        for i, ref in enumerate(comparison_result["unique_doc1"]):
                            st.markdown(f"**{i+1}.** {ref}")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà 2
                if comparison_result["unique_doc2"]:
                    with st.expander(f"‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà 2: {doc2} ({len(comparison_result['unique_doc2'])})", expanded=False):
                        for i, ref in enumerate(comparison_result["unique_doc2"]):
                            st.markdown(f"**{i+1}.** {ref}")
        else:
            st.warning("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö")
    else:
        st.info("‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö")

st.write("---")
st.write("‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: 9 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2568")
st.write("‡∏ú‡∏π‡πâ‡∏û‡∏±‡∏í‡∏ô‡∏≤: ‡∏≠‡∏ô‡∏∏‡∏™‡∏£‡∏ì‡πå ‡πÉ‡∏à‡πÅ‡∏Å‡πâ‡∏ß")

st.write("---")